{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1HhmnPsU9z2D",
        "GxV881bA-Rfk",
        "3tnPr0IO0r2Y",
        "mwfAgNQxLxqZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "Run this code to prepare notebook\n",
        "It defines functions and imports libraries"
      ],
      "metadata": {
        "id": "1HhmnPsU9z2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IfSO_NabSrw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "a38cffd2-17e8-4149-f808-c433b3ce1acf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe alpha for the 1 sigma confidence level\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# encoding: utf-8\n",
        "\n",
        "\"\"\"\n",
        "code based on modules from pypercolate\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import (absolute_import, division,\n",
        "                        print_function, unicode_literals)\n",
        "\n",
        "import scipy\n",
        "import timeit\n",
        "import copy\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "alpha_1sigma = 2 * scipy.stats.norm.cdf(-2.0)\n",
        "\"\"\"\n",
        "The alpha for the 1 sigma confidence level\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _microcanonical_average_spanning_cluster(has_spanning_cluster, alpha):\n",
        "    r'''\n",
        "    Compute the average number of runs that have a spanning cluster\n",
        "\n",
        "    Helper function for :func:`microcanonical_averages`\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    has_spanning_cluster : 1-D :py:class:`numpy.ndarray` of bool\n",
        "        Each entry is the ``has_spanning_cluster`` field of the output of\n",
        "        :func:`sample_states`:\n",
        "        An entry is ``True`` if there is a spanning cluster in that respective\n",
        "        run, and ``False`` otherwise.\n",
        "\n",
        "    alpha : float\n",
        "        Significance level.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    ret : dict\n",
        "        Spanning cluster statistics\n",
        "\n",
        "    ret['spanning_cluster'] : float\n",
        "        The average relative number (Binomial proportion) of runs that have a\n",
        "        spanning cluster.\n",
        "        This is the Bayesian point estimate of the posterior mean, with a\n",
        "        uniform prior.\n",
        "\n",
        "    ret['spanning_cluster_ci'] : 1-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        The lower and upper bounds of the Binomial proportion confidence\n",
        "        interval with uniform prior.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    sample_states : spanning cluster detection\n",
        "\n",
        "    microcanonical_averages : spanning cluster statistics\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "\n",
        "    Averages and confidence intervals for Binomial proportions\n",
        "\n",
        "    As Cameron [8]_ puts it, the normal approximation to the confidence\n",
        "    interval for a Binomial proportion :math:`p` \"suffers a *systematic*\n",
        "    decline in performance (...) towards extreme values of :math:`p` near\n",
        "    :math:`0` and :math:`1`, generating binomial [confidence intervals]\n",
        "    with effective coverage far below the desired level.\" (see also\n",
        "    References [6]_ and [7]_).\n",
        "\n",
        "    A different approach to quantifying uncertainty is Bayesian inference.\n",
        "    [5]_\n",
        "    For :math:`n` independent Bernoulli trails with common success\n",
        "    probability :math:`p`, the *likelihood* to have :math:`k` successes\n",
        "    given :math:`p` is the binomial distribution\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        P(k|p) = \\binom{n}{k} p^k (1-p)^{n-k} \\equiv B(a,b),\n",
        "\n",
        "    where :math:`B(a, b)` is the *Beta distribution* with parameters\n",
        "    :math:`a = k + 1` and :math:`b = n - k + 1`.\n",
        "    Assuming a uniform prior :math:`P(p) = 1`, the *posterior* is [5]_\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        P(p|k) = P(k|p)=B(a,b).\n",
        "\n",
        "    A point estimate is the posterior mean\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\bar{p} = \\frac{k+1}{n+2}\n",
        "\n",
        "    with the :math:`1 - \\alpha` credible interval :math:`(p_l, p_u)` given\n",
        "    by\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\int_0^{p_l} dp B(a,b) = \\int_{p_u}^1 dp B(a,b) = \\frac{\\alpha}{2}.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "\n",
        "    .. [5] Wasserman, L. All of Statistics (Springer New York, 2004),\n",
        "       `doi:10.1007/978-0-387-21736-9 <http://dx.doi.org/10.1007/978-0-387-21736-9>`_.\n",
        "\n",
        "    .. [6] DasGupta, A., Cai, T. T. & Brown, L. D. Interval Estimation for a\n",
        "       Binomial Proportion. Statistical Science 16, 101-133 (2001).\n",
        "       `doi:10.1214/ss/1009213286 <http://dx.doi.org/10.1214/ss/1009213286>`_.\n",
        "\n",
        "    .. [7] Agresti, A. & Coull, B. A. Approximate is Better than \"Exact\" for\n",
        "       Interval Estimation of Binomial Proportions. The American Statistician\n",
        "       52, 119-126 (1998),\n",
        "       `doi:10.2307/2685469 <http://dx.doi.org/10.2307/2685469>`_.\n",
        "\n",
        "    .. [8] Cameron, E. On the Estimation of Confidence Intervals for Binomial\n",
        "       Population Proportions in Astronomy: The Simplicity and Superiority of\n",
        "       the Bayesian Approach. Publications of the Astronomical Society of\n",
        "       Australia 28, 128-139 (2011),\n",
        "       `doi:10.1071/as10046 <http://dx.doi.org/10.1071/as10046>`_.\n",
        "\n",
        "    '''\n",
        "\n",
        "    ret = dict()\n",
        "    runs = has_spanning_cluster.size\n",
        "\n",
        "    # Bayesian posterior mean for Binomial proportion (uniform prior)\n",
        "    k = has_spanning_cluster.sum(dtype=float)\n",
        "    ret['spanning_cluster'] = ((k + 1) / (runs + 2))\n",
        "\n",
        "    # Bayesian credible interval for Binomial proportion (uniform\n",
        "    # prior)\n",
        "    ret['spanning_cluster_ci'] = scipy.stats.beta.ppf([alpha / 2, 1 - alpha / 2], k + 1, runs - k + 1)\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "nVsbK5DI_Vzj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _microcanonical_average_max_cluster_size(max_cluster_size, alpha):\n",
        "    \"\"\"\n",
        "    Compute the average size of the largest cluster\n",
        "\n",
        "    Helper function for :func:`microcanonical_averages`\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    max_cluster_size : 1-D :py:class:`numpy.ndarray` of int\n",
        "        Each entry is the ``max_cluster_size`` field of the output of\n",
        "        :func:`sample_states`:\n",
        "        The size of the largest cluster (absolute number of sites).\n",
        "\n",
        "    alpha: float\n",
        "        Significance level.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    ret : dict\n",
        "        Largest cluster statistics\n",
        "\n",
        "    ret['max_cluster_size'] : float\n",
        "        Average size of the largest cluster (absolute number of sites)\n",
        "\n",
        "    ret['max_cluster_size_ci'] : 1-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        Lower and upper bounds of the normal confidence interval of the average\n",
        "        size of the largest cluster (absolute number of sites)\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    sample_states : largest cluster detection\n",
        "\n",
        "    microcanonical_averages : largest cluster statistics\n",
        "    \"\"\"\n",
        "\n",
        "    ret = dict()\n",
        "    runs = max_cluster_size.size\n",
        "    sqrt_n = np.sqrt(runs)\n",
        "\n",
        "    max_cluster_size_sample_mean = max_cluster_size.mean()\n",
        "    ret['max_cluster_size'] = max_cluster_size_sample_mean\n",
        "\n",
        "    max_cluster_size_sample_std = max_cluster_size.std(ddof=1)\n",
        "    if max_cluster_size_sample_std:\n",
        "        old_settings = np.seterr(all='raise')\n",
        "        ret['max_cluster_size_ci'] = scipy.stats.t.interval(1 - alpha, df=runs - 1, loc=max_cluster_size_sample_mean,\n",
        "            scale=max_cluster_size_sample_std / sqrt_n)\n",
        "        np.seterr(**old_settings)\n",
        "    else:\n",
        "        ret['max_cluster_size_ci'] = (max_cluster_size_sample_mean * np.ones(2))\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "ZPsaXzGw_Qt9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _microcanonical_average_moments(moments, alpha):\n",
        "    \"\"\"\n",
        "    Compute the average moments of the cluster size distributions\n",
        "\n",
        "    Helper function for :func:`microcanonical_averages`\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    moments : 2-D :py:class:`numpy.ndarray` of int\n",
        "        ``moments.shape[1] == 5`.\n",
        "        Each array ``moments[r, :]`` is the ``moments`` field of the output of\n",
        "        :func:`sample_states`:\n",
        "        The ``k``-th entry is the ``k``-th raw moment of the (absolute) cluster\n",
        "        size distribution.\n",
        "\n",
        "    alpha: float\n",
        "        Significance level.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    ret : dict\n",
        "        Moment statistics\n",
        "\n",
        "    ret['moments'] : 1-D :py:class:`numpy.ndarray` of float, size 5\n",
        "        The ``k``-th entry is the average ``k``-th raw moment of the (absolute)\n",
        "        cluster size distribution, with ``k`` ranging from ``0`` to ``4``.\n",
        "\n",
        "    ret['moments_ci'] : 2-D :py:class:`numpy.ndarray` of float, shape (5,2)\n",
        "        ``ret['moments_ci'][k]`` are the lower and upper bounds of the normal\n",
        "        confidence interval of the average ``k``-th raw moment of the\n",
        "        (absolute) cluster size distribution, with ``k`` ranging from ``0`` to\n",
        "        ``4``.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    sample_states : computation of moments\n",
        "\n",
        "    microcanonical_averages : moment statistics\n",
        "    \"\"\"\n",
        "\n",
        "    ret = dict()\n",
        "    runs = moments.shape[0]\n",
        "    sqrt_n = np.sqrt(runs)\n",
        "\n",
        "    moments_sample_mean = moments.mean(axis=0)\n",
        "    ret['moments'] = moments_sample_mean\n",
        "\n",
        "    moments_sample_std = moments.std(axis=0, ddof=1)\n",
        "    ret['moments_ci'] = np.empty((5, 2))\n",
        "    for k in range(5):\n",
        "        if moments_sample_std[k]:\n",
        "            old_settings = np.seterr(all='raise')\n",
        "            ret['moments_ci'][k] = scipy.stats.t.interval(1 - alpha, df=runs - 1, loc=moments_sample_mean[k],\n",
        "                scale=moments_sample_std[k] / sqrt_n)\n",
        "            np.seterr(**old_settings)\n",
        "        else:\n",
        "            ret['moments_ci'][k] = (moments_sample_mean[k] * np.ones(2))\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "77gmoEaj_NcZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spanning_1d_chain(length):\n",
        "    \"\"\"\n",
        "    Generate a linear chain with auxiliary nodes for spanning cluster detection\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    length : int\n",
        "       Number of nodes in the chain, excluding the auxiliary nodes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    networkx.Graph\n",
        "       A linear chain graph with auxiliary nodes for spanning cluster detection\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    sample_states : spanning cluster detection\n",
        "\n",
        "    \"\"\"\n",
        "    ret = nx.grid_graph(dim=[int(length + 2)])\n",
        "\n",
        "    ret.nodes[0]['span'] = 0\n",
        "    ret[0][1]['span'] = 0\n",
        "    ret.nodes[length + 1]['span'] = 1\n",
        "    ret[length][length + 1]['span'] = 1\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "JhHDO0Hn_Hpd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def microcanonical_averages(\n",
        "    graph, runs=40, spanning_cluster=True, model='bond', alpha=alpha_1sigma,\n",
        "    copy_result=True\n",
        "):\n",
        "    r'''\n",
        "    Generate successive microcanonical percolation ensemble averages\n",
        "\n",
        "    This is a :ref:`generator function <python:tut-generators>` to successively\n",
        "    add one edge at a time from the graph to the percolation model for a number\n",
        "    of independent runs in parallel.\n",
        "    At each iteration, it calculates and returns the averaged cluster\n",
        "    statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    graph : networkx.Graph\n",
        "        The substrate graph on which percolation is to take place\n",
        "\n",
        "    runs : int, optional\n",
        "        Number of independent runs.\n",
        "        Defaults to ``40``.\n",
        "\n",
        "    spanning_cluster : bool, optional\n",
        "        Defaults to ``True``.\n",
        "\n",
        "    model : str, optional\n",
        "        The percolation model (either ``'bond'`` or ``'site'``).\n",
        "        Defaults to ``'bond'``.\n",
        "\n",
        "        .. note:: Other models than ``'bond'`` are not supported yet.\n",
        "\n",
        "    alpha: float, optional\n",
        "        Significance level.\n",
        "        Defaults to 1 sigma of the normal distribution.\n",
        "        ``1 - alpha`` is the confidence level.\n",
        "\n",
        "    copy_result : bool, optional\n",
        "        Whether to return a copy or a reference to the result dictionary.\n",
        "        Defaults to ``True``.\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    ret : dict\n",
        "        Cluster statistics\n",
        "\n",
        "    ret['n'] : int\n",
        "        Number of occupied bonds\n",
        "\n",
        "    ret['N'] : int\n",
        "        Total number of sites\n",
        "\n",
        "    ret['M'] : int\n",
        "        Total number of bonds\n",
        "\n",
        "    ret['spanning_cluster'] : float\n",
        "        The average number (Binomial proportion) of runs that have a spanning\n",
        "        cluster.\n",
        "        This is the Bayesian point estimate of the posterior mean, with a\n",
        "        uniform prior.\n",
        "        Only exists if `spanning_cluster` is set to ``True``.\n",
        "\n",
        "    ret['spanning_cluster_ci'] : 1-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        The lower and upper bounds of the Binomial proportion confidence\n",
        "        interval with uniform prior.\n",
        "        Only exists if `spanning_cluster` is set to ``True``.\n",
        "\n",
        "    ret['max_cluster_size'] : float\n",
        "        Average size of the largest cluster (absolute number of sites)\n",
        "\n",
        "    ret['max_cluster_size_ci'] : 1-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        Lower and upper bounds of the normal confidence interval of the average\n",
        "        size of the largest cluster (absolute number of sites)\n",
        "\n",
        "    ret['moments'] : 1-D :py:class:`numpy.ndarray` of float, size 5\n",
        "        The ``k``-th entry is the average ``k``-th raw moment of the (absolute)\n",
        "        cluster size distribution, with ``k`` ranging from ``0`` to ``4``.\n",
        "\n",
        "    ret['moments_ci'] : 2-D :py:class:`numpy.ndarray` of float, shape (5,2)\n",
        "        ``ret['moments_ci'][k]`` are the lower and upper bounds of the normal\n",
        "        confidence interval of the average ``k``-th raw moment of the\n",
        "        (absolute) cluster size distribution, with ``k`` ranging from ``0`` to\n",
        "        ``4``.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If `runs` is not a positive integer\n",
        "\n",
        "    ValueError\n",
        "        If `alpha` is not a float in the interval (0, 1)\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "\n",
        "    sample_states\n",
        "\n",
        "    percolate.percolate._microcanonical_average_spanning_cluster\n",
        "\n",
        "    percolate.percolate._microcanonical_average_max_cluster_size\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Iterating through this generator corresponds to several parallel runs of\n",
        "    the Newman-Ziff algorithm.\n",
        "    Each iteration yields a microcanonical percolation ensemble for the number\n",
        "    :math:`n` of occupied bonds. [9]_\n",
        "    The first iteration yields the trivial microcanonical percolation ensemble\n",
        "    with :math:`n = 0` occupied bonds.\n",
        "\n",
        "    Spanning cluster\n",
        "\n",
        "        .. seealso:: :py:func:`sample_states`\n",
        "\n",
        "    Raw moments of the cluster size distribution\n",
        "\n",
        "        .. seealso:: :py:func:`sample_states`\n",
        "\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [9] Newman, M. E. J. & Ziff, R. M. Fast monte carlo algorithm for site\n",
        "        or bond percolation. Physical Review E 64, 016706+ (2001),\n",
        "        `doi:10.1103/physreve.64.016706 <http://dx.doi.org/10.1103/physreve.64.016706>`_.\n",
        "\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        runs = int(runs)\n",
        "    except:\n",
        "        raise ValueError(\"runs needs to be a positive integer\")\n",
        "\n",
        "    if runs <= 0:\n",
        "        raise ValueError(\"runs needs to be a positive integer\")\n",
        "\n",
        "    try:\n",
        "        alpha = float(alpha)\n",
        "    except:\n",
        "        raise ValueError(\"alpha needs to be a float in the interval (0, 1)\")\n",
        "\n",
        "    if alpha <= 0.0 or alpha >= 1.0:\n",
        "        raise ValueError(\"alpha needs to be a float in the interval (0, 1)\")\n",
        "\n",
        "    # initial iteration\n",
        "    # we do not need a copy of the result dictionary since we copy the values\n",
        "    # anyway\n",
        "    run_iterators = [sample_states(graph, spanning_cluster=spanning_cluster, model=model, copy_result=False)\n",
        "        for _ in range(runs)]\n",
        "\n",
        "    ret = dict()\n",
        "    for microcanonical_ensemble in zip(*run_iterators):\n",
        "        # merge cluster statistics\n",
        "        ret['n'] = microcanonical_ensemble[0]['n']\n",
        "        ret['N'] = microcanonical_ensemble[0]['N']\n",
        "        ret['M'] = microcanonical_ensemble[0]['M']\n",
        "\n",
        "        max_cluster_size = np.empty(runs)\n",
        "        moments = np.empty((runs, 5))\n",
        "        if spanning_cluster:\n",
        "            has_spanning_cluster = np.empty(runs)\n",
        "\n",
        "        for r, state in enumerate(microcanonical_ensemble):\n",
        "            assert state['n'] == ret['n']\n",
        "            assert state['N'] == ret['N']\n",
        "            assert state['M'] == ret['M']\n",
        "            max_cluster_size[r] = state['max_cluster_size']\n",
        "            moments[r] = state['moments']\n",
        "            if spanning_cluster:\n",
        "                has_spanning_cluster[r] = state['has_spanning_cluster']\n",
        "\n",
        "        ret.update(_microcanonical_average_max_cluster_size(max_cluster_size, alpha))\n",
        "\n",
        "        ret.update(_microcanonical_average_moments(moments, alpha))\n",
        "\n",
        "        if spanning_cluster:\n",
        "            ret.update(_microcanonical_average_spanning_cluster(has_spanning_cluster, alpha))\n",
        "\n",
        "        if copy_result:\n",
        "            yield copy.deepcopy(ret)\n",
        "        else:\n",
        "            yield ret"
      ],
      "metadata": {
        "id": "07ENGl5L-cdh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_states(\n",
        "    graph, spanning_cluster=True, model='bond', copy_result=True\n",
        "):\n",
        "    '''\n",
        "    Generate successive sample states of the percolation model\n",
        "\n",
        "    This is a :ref:`generator function <python:tut-generators>` to successively\n",
        "    add one edge at a time from the graph to the percolation model.\n",
        "    At each iteration, it calculates and returns the cluster statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    graph : networkx.Graph\n",
        "        The substrate graph on which percolation is to take place\n",
        "\n",
        "    spanning_cluster : bool, optional\n",
        "        Whether to detect a spanning cluster or not.\n",
        "        Defaults to ``True``.\n",
        "\n",
        "    model : str, optional\n",
        "        The percolation model (either ``'bond'`` or ``'site'``).\n",
        "        Defaults to ``'bond'``.\n",
        "\n",
        "        .. note:: Other models than ``'bond'`` are not supported yet.\n",
        "\n",
        "    copy_result : bool, optional\n",
        "        Whether to return a copy or a reference to the result dictionary.\n",
        "        Defaults to ``True``.\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    ret : dict\n",
        "        Cluster statistics\n",
        "\n",
        "    ret['n'] : int\n",
        "        Number of occupied bonds\n",
        "\n",
        "    ret['N'] : int\n",
        "        Total number of sites\n",
        "\n",
        "    ret['M'] : int\n",
        "        Total number of bonds\n",
        "\n",
        "    ret['has_spanning_cluster'] : bool\n",
        "        ``True`` if there is a spanning cluster, ``False`` otherwise.\n",
        "        Only exists if `spanning_cluster` argument is set to ``True``.\n",
        "\n",
        "    ret['max_cluster_size'] : int\n",
        "        Size of the largest cluster (absolute number of sites)\n",
        "\n",
        "    ret['moments'] : 1-D :py:class:`numpy.ndarray` of int\n",
        "        Array of size ``5``.\n",
        "        The ``k``-th entry is the ``k``-th raw moment of the (absolute) cluster\n",
        "        size distribution, with ``k`` ranging from ``0`` to ``4``.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If `model` does not equal ``'bond'``.\n",
        "\n",
        "    ValueError\n",
        "        If `spanning_cluster` is ``True``, but `graph` does not contain any\n",
        "        auxiliary nodes to detect spanning clusters.\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "\n",
        "    microcanonical_averages : Evolves multiple sample states in parallel\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Iterating through this generator is a single run of the Newman-Ziff\n",
        "    algorithm. [2]_\n",
        "    The first iteration yields the trivial state with :math:`n = 0` occupied\n",
        "    bonds.\n",
        "\n",
        "    Spanning cluster\n",
        "\n",
        "        In order to detect a spanning cluster, `graph` needs to contain\n",
        "        auxiliary nodes and edges, cf. Reference [2]_, Figure 6.\n",
        "        The auxiliary nodes and edges have the ``'span'`` `attribute\n",
        "        <http://networkx.github.io/documentation/latest/tutorial/tutorial.html#node-attributes>`_.\n",
        "        The value is either ``0`` or ``1``, distinguishing the two sides of the\n",
        "        graph to span.\n",
        "\n",
        "    Raw moments of the cluster size distribution\n",
        "\n",
        "        The :math:`k`-th raw moment of the (absolute) cluster size distribution\n",
        "        is :math:`\\sum_s' s^k N_s`, where :math:`s` is the cluster size and\n",
        "        :math:`N_s` is the number of clusters of size :math:`s`. [3]_\n",
        "        The primed sum :math:`\\sum'` signifies that the largest cluster is\n",
        "        excluded from the sum. [4]_\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [2] Newman, M. E. J. & Ziff, R. M. Fast monte carlo algorithm for site\n",
        "        or bond percolation. Physical Review E 64, 016706+ (2001),\n",
        "        `doi:10.1103/physreve.64.016706 <http://dx.doi.org/10.1103/physreve.64.016706>`_.\n",
        "\n",
        "    .. [3] Stauffer, D. & Aharony, A. Introduction to Percolation Theory (Taylor &\n",
        "       Francis, London, 1994), second edn.\n",
        "\n",
        "    .. [4] Binder, K. & Heermann, D. W. Monte Carlo Simulation in Statistical\n",
        "       Physics (Springer, Berlin, Heidelberg, 2010),\n",
        "       `doi:10.1007/978-3-642-03163-2 <http://dx.doi.org/10.1007/978-3-642-03163-2>`_.\n",
        "    '''\n",
        "\n",
        "    if model != 'bond':\n",
        "        raise ValueError('Only bond percolation supported.')\n",
        "\n",
        "    if spanning_cluster:\n",
        "        auxiliary_node_attributes = nx.get_node_attributes(graph, 'span')\n",
        "        auxiliary_nodes = auxiliary_node_attributes.keys()\n",
        "        if not list(auxiliary_nodes):\n",
        "            raise ValueError('Spanning cluster is to be detected, but no auxiliary nodes given.')\n",
        "\n",
        "        spanning_sides = list(set(auxiliary_node_attributes.values()))\n",
        "        if len(spanning_sides) != 2:\n",
        "            raise ValueError('Spanning cluster is to be detected, but auxiliary nodes '\n",
        "                'of less or more than 2 types (sides) given.')\n",
        "\n",
        "        auxiliary_edge_attributes = nx.get_edge_attributes(graph, 'span')\n",
        "\n",
        "    # get subgraph on which percolation is to take place (strip off the auxiliary nodes)\n",
        "    if spanning_cluster:\n",
        "        perc_graph = graph.subgraph([node for node in graph.__iter__() if 'span' not in graph.nodes[node]])\n",
        "    else:\n",
        "        perc_graph = graph\n",
        "\n",
        "    # get a list of edges for easy access in later iterations\n",
        "    perc_edges = perc_graph.edges()\n",
        "\n",
        "    # number of nodes N\n",
        "    num_nodes = nx.number_of_nodes(perc_graph)\n",
        "\n",
        "    # number of edges M\n",
        "    num_edges = nx.number_of_edges(perc_graph)\n",
        "\n",
        "    # initial iteration: no edges added yet (n == 0)\n",
        "    ret = dict()\n",
        "\n",
        "    ret['n'] = 0\n",
        "    ret['N'] = num_nodes\n",
        "    ret['M'] = num_edges\n",
        "    ret['max_cluster_size'] = 1\n",
        "    ret['moments'] = np.ones(5) * (num_nodes - 1)\n",
        "\n",
        "    if spanning_cluster:\n",
        "        ret['has_spanning_cluster'] = False\n",
        "\n",
        "    if copy_result:\n",
        "        yield copy.deepcopy(ret)\n",
        "    else:\n",
        "        yield ret\n",
        "\n",
        "    # permute edges\n",
        "    perm_edges = np.random.permutation(num_edges)\n",
        "\n",
        "    # set up disjoint set (union-find) data structure\n",
        "    ds = nx.utils.union_find.UnionFind()\n",
        "    if spanning_cluster:\n",
        "        ds_spanning = nx.utils.union_find.UnionFind()\n",
        "\n",
        "        # merge all auxiliary nodes for each side\n",
        "        side_roots = dict()\n",
        "        for side in spanning_sides:\n",
        "            nodes = [node for (node, node_side) in auxiliary_node_attributes.items() if node_side is side]\n",
        "            ds_spanning.union(*nodes)\n",
        "            side_roots[side] = ds_spanning[nodes[0]]\n",
        "\n",
        "        for (edge, edge_side) in auxiliary_edge_attributes.items():\n",
        "            ds_spanning.union(side_roots[edge_side], *edge)\n",
        "\n",
        "        side_roots = [ds_spanning[side_root] for side_root in side_roots.values()]\n",
        "\n",
        "    # get first node\n",
        "    max_cluster_root = next(perc_graph.__iter__())\n",
        "\n",
        "    # loop over all edges (n == 1..M)\n",
        "    for n in range(num_edges):\n",
        "        ret['n'] = n + 1\n",
        "\n",
        "        # draw new edge from permutation\n",
        "        edge_index = perm_edges[n]\n",
        "        perc_edges = np.array(perc_edges)\n",
        "        edge = perc_edges[edge_index]\n",
        "        ret['edge'] = edge\n",
        "\n",
        "        # find roots and weights\n",
        "        #print(type(ds))\n",
        "        #ds = np.array(ds)\n",
        "        #print(ds)\n",
        "        roots = [ds[node] for node in edge]\n",
        "        weights = [ds.weights[root] for root in roots]\n",
        "\n",
        "        if roots[0] is not roots[1]:\n",
        "            # not same cluster: union!\n",
        "            ds.union(*roots)\n",
        "            if spanning_cluster:\n",
        "                ds_spanning.union(*roots)\n",
        "\n",
        "                ret['has_spanning_cluster'] = (ds_spanning[side_roots[0]] == ds_spanning[side_roots[1]])\n",
        "\n",
        "            # find new root and weight\n",
        "            root = ds[edge[0]]\n",
        "            weight = ds.weights[root]\n",
        "\n",
        "            # moments and maximum cluster size\n",
        "\n",
        "            # deduct the previous sub-maximum clusters from moments\n",
        "            for i in [0, 1]:\n",
        "                if roots[i] is max_cluster_root:\n",
        "                    continue\n",
        "                ret['moments'] -= weights[i] ** np.arange(5)\n",
        "\n",
        "            if max_cluster_root in roots:\n",
        "                # merged with maximum cluster\n",
        "                max_cluster_root = root\n",
        "                ret['max_cluster_size'] = weight\n",
        "            else:\n",
        "                # merged previously sub-maximum clusters\n",
        "                if ret['max_cluster_size'] >= weight:\n",
        "                    # previously largest cluster remains largest cluster\n",
        "                    # add merged cluster to moments\n",
        "                    ret['moments'] += weight ** np.arange(5)\n",
        "                else:\n",
        "                    # merged cluster overtook previously largest cluster\n",
        "                    # add previously largest cluster to moments\n",
        "                    max_cluster_root = root\n",
        "                    ret['moments'] += ret['max_cluster_size'] ** np.arange(5)\n",
        "                    ret['max_cluster_size'] = weight\n",
        "\n",
        "        if copy_result:\n",
        "            yield copy.deepcopy(ret)\n",
        "        else:\n",
        "            yield ret"
      ],
      "metadata": {
        "id": "8Xo7s94v-Dng"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def microcanonical_averages_arrays(microcanonical_averages):\n",
        "    \"\"\"\n",
        "    Compile microcanonical averages over all iteration steps into single arrays\n",
        "\n",
        "    Helper function to aggregate the microcanonical averages over all iteration\n",
        "    steps into single arrays for further processing\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    microcanonical_averages : iterable\n",
        "       Typically, this is the :func:`microcanonical_averages` generator\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    ret : dict\n",
        "       Aggregated cluster statistics\n",
        "\n",
        "    ret['N'] : int\n",
        "        Total number of sites\n",
        "\n",
        "    ret['M'] : int\n",
        "        Total number of bonds\n",
        "\n",
        "    ret['spanning_cluster'] : 1-D :py:class:`numpy.ndarray` of float\n",
        "        The percolation probability:\n",
        "        The normalized average number of runs that have a spanning cluster.\n",
        "\n",
        "    ret['spanning_cluster_ci'] : 2-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        The lower and upper bounds of the percolation probability.\n",
        "\n",
        "    ret['max_cluster_size'] : 1-D :py:class:`numpy.ndarray` of float\n",
        "        The percolation strength:\n",
        "        Average relative size of the largest cluster\n",
        "\n",
        "    ret['max_cluster_size_ci'] : 2-D :py:class:`numpy.ndarray` of float\n",
        "        Lower and upper bounds of the normal confidence interval of the\n",
        "        percolation strength.\n",
        "\n",
        "    ret['moments'] : 2-D :py:class:`numpy.ndarray` of float, shape (5, M + 1)\n",
        "        Average raw moments of the (relative) cluster size distribution.\n",
        "\n",
        "    ret['moments_ci'] : 3-D :py:class:`numpy.ndarray` of float, shape (5, M + 1, 2)\n",
        "        Lower and upper bounds of the normal confidence interval of the raw\n",
        "        moments of the (relative) cluster size distribution.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    microcanonical_averages\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ret = dict()\n",
        "\n",
        "    for n, microcanonical_average in enumerate(microcanonical_averages):\n",
        "        assert n == microcanonical_average['n']\n",
        "        if n == 0:\n",
        "            num_edges = microcanonical_average['M']\n",
        "            num_sites = microcanonical_average['N']\n",
        "            spanning_cluster = ('spanning_cluster' in microcanonical_average)\n",
        "            ret['max_cluster_size'] = np.empty(num_edges + 1)\n",
        "            ret['max_cluster_size_ci'] = np.empty((num_edges + 1, 2))\n",
        "\n",
        "            if spanning_cluster:\n",
        "                ret['spanning_cluster'] = np.empty(num_edges + 1)\n",
        "                ret['spanning_cluster_ci'] = np.empty((num_edges + 1, 2))\n",
        "\n",
        "            ret['moments'] = np.empty((5, num_edges + 1))\n",
        "            ret['moments_ci'] = np.empty((5, num_edges + 1, 2))\n",
        "\n",
        "        ret['max_cluster_size'][n] = microcanonical_average['max_cluster_size']\n",
        "        ret['max_cluster_size_ci'][n] = (microcanonical_average['max_cluster_size_ci'])\n",
        "\n",
        "        if spanning_cluster:\n",
        "            ret['spanning_cluster'][n] = (microcanonical_average['spanning_cluster'])\n",
        "            ret['spanning_cluster_ci'][n] = (microcanonical_average['spanning_cluster_ci'])\n",
        "\n",
        "        ret['moments'][:, n] = microcanonical_average['moments']\n",
        "        ret['moments_ci'][:, n] = microcanonical_average['moments_ci']\n",
        "\n",
        "    # normalize by number of sites\n",
        "    for key in ret:\n",
        "        if 'spanning_cluster' in key:\n",
        "            continue\n",
        "        ret[key] /= num_sites\n",
        "\n",
        "    ret['M'] = num_edges\n",
        "    ret['N'] = num_sites\n",
        "    return ret\n"
      ],
      "metadata": {
        "id": "zcEB0v0F-Rhl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def canonical_averages(ps, microcanonical_averages_arrays):\n",
        "    \"\"\"\n",
        "    Compute the canonical cluster statistics from microcanonical statistics\n",
        "\n",
        "    This is according to Newman and Ziff, Equation (2).\n",
        "    Note that we also simply average the bounds of the confidence intervals\n",
        "    according to this formula.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    ps : iterable of float\n",
        "       Each entry is a probability for which to form the canonical ensemble\n",
        "       and compute the weighted statistics from the microcanonical statistics\n",
        "\n",
        "    microcanonical_averages_arrays\n",
        "       Typically the output of :func:`microcanonical_averages_arrays`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    ret : dict\n",
        "       Canonical ensemble cluster statistics\n",
        "\n",
        "    ret['ps'] : iterable of float\n",
        "        The parameter `ps`\n",
        "\n",
        "    ret['N'] : int\n",
        "        Total number of sites\n",
        "\n",
        "    ret['M'] : int\n",
        "        Total number of bonds\n",
        "\n",
        "    ret['spanning_cluster'] : 1-D :py:class:`numpy.ndarray` of float\n",
        "        The percolation probability:\n",
        "        The normalized average number of runs that have a spanning cluster.\n",
        "\n",
        "    ret['spanning_cluster_ci'] : 2-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        The lower and upper bounds of the percolation probability.\n",
        "\n",
        "    ret['max_cluster_size'] : 1-D :py:class:`numpy.ndarray` of float\n",
        "        The percolation strength:\n",
        "        Average relative size of the largest cluster\n",
        "\n",
        "    ret['max_cluster_size_ci'] : 2-D :py:class:`numpy.ndarray` of float\n",
        "        Lower and upper bounds of the normal confidence interval of the\n",
        "        percolation strength.\n",
        "\n",
        "    ret['moments'] : 2-D :py:class:`numpy.ndarray` of float, shape (5, M + 1)\n",
        "        Average raw moments of the (relative) cluster size distribution.\n",
        "\n",
        "    ret['moments_ci'] : 3-D :py:class:`numpy.ndarray` of float, shape (5, M + 1, 2)\n",
        "        Lower and upper bounds of the normal confidence interval of the raw\n",
        "        moments of the (relative) cluster size distribution.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    microcanonical_averages\n",
        "\n",
        "    microcanonical_averages_arrays\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    num_sites = microcanonical_averages_arrays['N']\n",
        "    num_edges = microcanonical_averages_arrays['M']\n",
        "    spanning_cluster = ('spanning_cluster' in microcanonical_averages_arrays)\n",
        "\n",
        "    ret = dict()\n",
        "    ret['ps'] = ps\n",
        "    ret['N'] = num_sites\n",
        "    ret['M'] = num_edges\n",
        "\n",
        "    ret['max_cluster_size'] = np.empty(ps.size)\n",
        "    ret['max_cluster_size_ci'] = np.empty((ps.size, 2))\n",
        "\n",
        "    if spanning_cluster:\n",
        "        ret['spanning_cluster'] = np.empty(ps.size)\n",
        "        ret['spanning_cluster_ci'] = np.empty((ps.size, 2))\n",
        "\n",
        "    ret['moments'] = np.empty((5, ps.size))\n",
        "    ret['moments_ci'] = np.empty((5, ps.size, 2))\n",
        "\n",
        "    for p_index, p in enumerate(ps):\n",
        "        binomials = _binomial_pmf(n=num_edges, p=p)\n",
        "\n",
        "        for key, value in microcanonical_averages_arrays.items():\n",
        "            if len(key) <= 1:\n",
        "                continue\n",
        "\n",
        "            if key in ['max_cluster_size', 'spanning_cluster']:\n",
        "                ret[key][p_index] = np.sum(binomials * value)\n",
        "            elif key in ['max_cluster_size_ci', 'spanning_cluster_ci']:\n",
        "                ret[key][p_index] = np.sum(np.tile(binomials, (2, 1)).T * value, axis=0)\n",
        "            elif key == 'moments':\n",
        "                ret[key][:, p_index] = np.sum(np.tile(binomials, (5, 1)) * value, axis=1)\n",
        "            elif key == 'moments_ci':\n",
        "                ret[key][:, p_index] = np.sum(np.rollaxis(np.tile(binomials, (5, 2, 1)), 2, 1) * value, axis=1)\n",
        "            else:\n",
        "                raise NotImplementedError('{}-dimensional array'.format(value.ndim))\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "hWi-wUui-1uW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _binomial_pmf(n, p):\n",
        "    \"\"\"\n",
        "    Compute the binomial PMF according to Newman and Ziff\n",
        "\n",
        "    Helper function for :func:`canonical_averages`\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    canonical_averages\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "\n",
        "    See Newman & Ziff, Equation (10) [10]_\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "\n",
        "    .. [10] Newman, M. E. J. & Ziff, R. M. Fast monte carlo algorithm for site\n",
        "        or bond percolation. Physical Review E 64, 016706+ (2001),\n",
        "        `doi:10.1103/physreve.64.016706 <http://dx.doi.org/10.1103/physreve.64.016706>`_.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n = int(n)\n",
        "    ret = np.empty(n + 1)\n",
        "\n",
        "    nmax = int(np.round(p * n))\n",
        "\n",
        "    ret[nmax] = 1.0\n",
        "\n",
        "    old_settings = np.seterr(under='ignore')  # seterr to known value\n",
        "\n",
        "    for i in range(nmax + 1, n + 1):\n",
        "        ret[i] = ret[i - 1] * (n - i + 1.0) / i * p / (1.0 - p)\n",
        "\n",
        "    for i in range(nmax - 1, -1, -1):\n",
        "        ret[i] = ret[i + 1] * (i + 1.0) / (n - i) * (1.0 - p) / p\n",
        "\n",
        "    ret = ret / ret.sum()\n",
        "    #np.seterr(**old_settings)  # reset to default\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "LoRpIqcp-89u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Canonical_Simulation(chain_runs,chain_ls,number_of_ps):\n",
        "  \"\"\"\n",
        "    This is the master function which pulls together all the other functions needed to simulate percolation\n",
        "    It creates arrays to store data; it then evolves the lattice and works with the microcanonical ensemble;\n",
        "    it then transforms the data back to the canonical ensemble\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    chain_runs: integer\n",
        "       This is the size of the size of the microcanonical ensemble used during the simulation\n",
        "\n",
        "    chain_ls: interable, containing integers\n",
        "       This object contains the lengths of each of the chains that are to be simulated\n",
        "\n",
        "    number_of_ps: integer\n",
        "       This decides how high the resolution the simulation is in probability space.\n",
        "       The number given here decides how many different values of probability to return results for\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    chain_ls: numpy array\n",
        "      This returns the list of chain legths for convenince\n",
        "\n",
        "    chain_ps_arrays: list\n",
        "      This returns the list of probability-values used in the simulation\n",
        "\n",
        "    chain_stats: list\n",
        "      This returns a list of the outputs for each level of zooming into the critical point\n",
        "\n",
        "    chain_stats[i]: list\n",
        "       A list of the data structures for each chain length\n",
        "\n",
        "    chain_stats[i][j]: dict\n",
        "       Canonical ensemble cluster statistics for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['ps'] : iterable of float\n",
        "        The parameter `ps` for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['N'] : int\n",
        "        Total number of sites for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['M'] : int\n",
        "        Total number of bonds for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['spanning_cluster'] : 1-D :py:class:`numpy.ndarray` of float\n",
        "        The percolation probability:\n",
        "        The normalized average number of runs that have a spanning cluster, for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['spanning_cluster_ci'] : 2-D :py:class:`numpy.ndarray` of float, size 2\n",
        "        The lower and upper bounds of the percolation probability, for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['max_cluster_size'] : 1-D :py:class:`numpy.ndarray` of float\n",
        "        The percolation strength:\n",
        "        Average relative size of the largest cluster, for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['max_cluster_size_ci'] : 2-D :py:class:`numpy.ndarray` of float\n",
        "        Lower and upper bounds of the normal confidence interval of the\n",
        "        percolation strength, for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['moments'] : 2-D :py:class:`numpy.ndarray` of float, shape (5, M + 1)\n",
        "        Average raw moments of the (relative) cluster size distribution, for probability interval i and chain length j\n",
        "\n",
        "    chain_stats[i][j]['moments_ci'] : 3-D :py:class:`numpy.ndarray` of float, shape (5, M + 1, 2)\n",
        "        Lower and upper bounds of the normal confidence interval of the raw\n",
        "        moments of the (relative) cluster size distribution, for probability interval i and chain length j\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "\n",
        "    spanning_1d_chain()\n",
        "    microcanonical_averages_arrays()\n",
        "    canonical_averages()\n",
        "\n",
        "  \"\"\"\n",
        "  chain_ls = np.array(chain_ls)\n",
        "  # generate the linear chain graphs with spanning cluster detection\n",
        "  # for all system sizes\n",
        "  chain_graphs = [spanning_1d_chain(l) for l in chain_ls ]\n",
        "\n",
        "  # compute the microcanonical averages for all system sizes\n",
        "  chain_microcanonical_averages = [microcanonical_averages(graph=chain_graph, runs=chain_runs) for chain_graph in chain_graphs]\n",
        "  # combine microcanonical averages into one array\n",
        "  chain_microcanonical_averages_arrays = [microcanonical_averages_arrays(avg) for avg in chain_microcanonical_averages]\n",
        "\n",
        "  # occupation probabilities\n",
        "  chain_ps_arrays = [ np.linspace(1.0 - x, 1.0, num=number_of_ps) for x in [1.0, 0.1, 0.01] ]\n",
        "\n",
        "  # compute canonical averages from microcanonical averages\n",
        "  # for all occupation probabilities and system sizes\n",
        "  chain_stats = [[canonical_averages(ps, avg_arrays) for avg_arrays in chain_microcanonical_averages_arrays] for ps in chain_ps_arrays]\n",
        "\n",
        "  return chain_ls, chain_ps_arrays, chain_stats"
      ],
      "metadata": {
        "id": "Te-3wHjbsHcA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Simulation and Plot Results\n",
        "Run a simulation and plot key statistics"
      ],
      "metadata": {
        "id": "GxV881bA-Rfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this cell generates data\n",
        "chain_ls,chain_ps_arrays, chain_stats = Canonical_Simulation(chain_runs = 50, chain_ls = [10, 100], number_of_ps = 100) #1000, 10000"
      ],
      "metadata": {
        "id": "J7rXSzQEJ3Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "fig, axes = plt.subplots(nrows=len(chain_ps_arrays), ncols=4, squeeze=True, figsize=(8.0, 4.5))\n",
        "\n",
        "for ps_index, ps in enumerate(chain_ps_arrays):\n",
        "    for l_index, l in enumerate(chain_ls):\n",
        "        my_stats = chain_stats[ps_index][l_index]\n",
        "\n",
        "        line, = axes[ps_index, 0].plot(ps, my_stats['spanning_cluster'], rasterized=True, label=r'{}'.format(l))\n",
        "\n",
        "        axes[ps_index, 0].fill_between(ps, my_stats['spanning_cluster_ci'].T[1], my_stats['spanning_cluster_ci'].T[0],\n",
        "            facecolor=line.get_color(), alpha=0.5, rasterized=True)\n",
        "\n",
        "        line, = axes[ps_index, 1].plot(ps, my_stats['max_cluster_size'], rasterized=True, label=r'L={}'.format(l))\n",
        "\n",
        "        axes[ps_index, 1].fill_between(ps, my_stats['max_cluster_size_ci'].T[1],\n",
        "            my_stats['max_cluster_size_ci'].T[0], facecolor=line.get_color(), alpha=0.5, rasterized=True)\n",
        "\n",
        "        axes[ps_index, 2].plot(ps, my_stats['moments'][2], rasterized=True, label=r'L={}'.format(l))\n",
        "\n",
        "        axes[ps_index, 2].fill_between(ps, my_stats['moments_ci'][2].T[1], my_stats['moments_ci'][2].T[0],\n",
        "            facecolor=line.get_color(), alpha=0.5, rasterized=True)\n",
        "\n",
        "        axes[ps_index, 3].semilogy(ps, my_stats['moments'][2], rasterized=True)\n",
        "\n",
        "        axes[ps_index, 3].fill_between(ps, np.where(my_stats['moments_ci'][2].T[1] > 0.0,\n",
        "                my_stats['moments_ci'][2].T[1],\n",
        "                0.01),\n",
        "            np.where(my_stats['moments_ci'][2].T[0] > 0.0, my_stats['moments_ci'][2].T[0],\n",
        "                0.01), facecolor=line.get_color(), alpha=0.5, rasterized=True)\n",
        "\n",
        "    axes[ps_index, 0].set_ylim(ymax=1.0)\n",
        "    axes[ps_index, 1].set_ylim(ymax=1.0)\n",
        "    axes[ps_index, 2].set_ylim(ymin=0.0)\n",
        "    axes[ps_index, 3].set_ylim(ymin=0.5)\n",
        "\n",
        "    for ax in axes[ps_index, :]:\n",
        "        ax.set_xlim(xmin=ps.min(), xmax=ps.max() + (ps.max() - ps.min()) * 0.05)\n",
        "        ax.set_xticks(np.linspace(ps.min(), ps.max(), num=3))\n",
        "\n",
        "    for ax in axes[ps_index, :-1]:\n",
        "        ax.set_yticks(np.linspace(0, ax.get_ylim()[1], num=3))\n",
        "\n",
        "\n",
        "axes[0, 0].set_title(r'Perc. Probability')\n",
        "axes[0, 1].set_title(r'Size of Max. Cluster')\n",
        "axes[0, 2].set_title(r'Mean Cluster Size')\n",
        "axes[0, 3].set_title(r'Mean Cluster Size')\n",
        "\n",
        "for ax in axes[-1, :]:\n",
        "    ax.set_xlabel(r'$p$')\n",
        "\n",
        "axes[0, 2].legend(frameon=False, loc='best')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('Percolation Fig 4.2.jpg', dpi = 600, format = 'jpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qqnOsgx3-QU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase Transition\n",
        "Find the critical values of p using prior data, and plot them\n"
      ],
      "metadata": {
        "id": "3tnPr0IO0r2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code approximates the position of the phase transition by finding the value of p for which the mean cluster size is maximised\n",
        "#These values of p are the crtical values of the system for the given system size\n",
        "k = 0\n",
        "values = []\n",
        "while k < len(chain_stats[0]):\n",
        " i = 0\n",
        " while i < len(chain_stats[0][k]['moments'][2]):\n",
        "  if max(abs(chain_stats[0][k]['moments'][2])) == abs(chain_stats[0][k]['moments'][2][i]):\n",
        "   values.append(round(chain_stats[0][k]['ps'][i],4))\n",
        "  i += 1\n",
        " k +=1\n",
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY8ntElkWutg",
        "outputId": "1ca072d3-e2b1-49a2-ed34-fe86f530aeb4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.596, 0.9596]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complexity\n",
        "Find the complexity of the system, and plot it"
      ],
      "metadata": {
        "id": "mwfAgNQxLxqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#containers for data\n",
        "sizes = np.linspace(50,1050,11)     #lengths of chain for percolation\n",
        "run_times = []\n",
        "\n",
        "#loop for timing the code\n",
        "for L in sizes:\n",
        " time_duration = timeit.timeit(f'Canonical_Simulation(100,{[L]}, number_of_ps = 100)', setup='from __main__ import Canonical_Simulation', number=20)\n",
        " run_times.append(time_duration)\n",
        "\n",
        "#define fit function\n",
        "def f(x,a,b):\n",
        "  return a*x**b\n",
        "\n",
        "(a_,b_), delta = scipy.optimize.curve_fit(f,sizes,run_times)    #curve fitting\n",
        "delta_a_ = np.sqrt(np.diag(delta))[0]\n",
        "delta_b_ = np.sqrt(np.diag(delta))[1]\n",
        "\n",
        "print('Time = a(L)^b')                              #display results\n",
        "print('a = ', a_, '+/-',delta_a_)\n",
        "print('a = ', b_, '+/-',delta_b_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqIZlxbbr-ln",
        "outputId": "d441286f-e9c0-4dee-f892-7448104ad902"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time = a(L)^b\n",
            "a =  0.23762006848620096 +/- 0.04904903438196744\n",
            "a =  0.9518312983627405 +/- 0.036577224212671566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell plots the it along with the original data\n",
        "timelinspace = np.linspace(0,max(sizes),200)\n",
        "plt.plot(timelinspace,f(timelinspace,a_,b_),color = 'black',label = f't = {round(a_,2)}*L^{round(b_,2)}')  #fitted curve\n",
        "plt.plot(sizes,run_times, color = 'red', label = 'Data', marker = 'x',linewidth = 0)    #raw data\n",
        "plt.xlim(0,max(sizes))\n",
        "plt.ylim(0,250)\n",
        "plt.xlabel('Number of Sites in Chain, L')\n",
        "plt.ylabel('Time taken for Percolation, t / s')\n",
        "plt.title('Plot of Time Taken Against Chain Length for 1-D Chain')\n",
        "plt.legend()\n",
        "plt.savefig('Percolation Fig 4.1.jpg', dpi = 600, format = 'jpg')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "7-aTtmtyweUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}